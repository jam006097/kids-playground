# AI API 開発ガイド

このドキュメントは、Hugging Face SpacesとGradioを利用したAI APIの開発環境構築と、その後の開発プロセスに関する要点をまとめたものです。

## 1. 概要

### 1.1. 目的と機能概要

本プロジェクトの目的は、「親子で遊ぼうナビ」にAIを活用した**口コミの自動要約機能**を実装することです。

- **目的:** ユーザーが多数の口コミを全て読まなくても、施設の全体的な評判を素早く、かつ客観的に把握できるようにする。
- **ユーザー体験:**
    1. ユーザーは施設の詳細ページで「口コミをAI要約」ボタンをクリックします。
    2. AIがその施設の全口コミを分析し、「ポジティブな点」と「注意が必要な点」などをまとめた中立的な要約文を生成します。
    3. 生成された要約がモーダルウィンドウ等で表示され、ユーザーは短時間で施設の長所と短所を理解できます。

### 1.2. アーキテクチャ

この機能は、Djangoアプリケーションと、Hugging Face Spaces上で動作するGradio APIとの連携によって実現します。

- **開発環境:** Dockerコンテナ内で開発・テストを行う。
- **バージョン管理:** Gitでコードを管理する。
- **CI/CD:** GitHub Actionsでテストとデプロイを自動化する。
- **本番環境:** Hugging Face Spacesにデプロイし、APIを公開する。

---

## 2. プロジェクト構造と開発環境

保守性・拡張性を高めるため、役割ごとにファイルを分割したディレクトリ構造を採用し、Dockerで開発環境を構築します。

### 2.1. 推奨ディレクトリ構造

```
kids-playground-ai-api/
│
├── .github/
│   └── workflows/
│       └── ci.yml
│
├── .gitignore
├── Dockerfile
├── pyproject.toml
├── requirements.txt
├── README.md
│
├── src/
│   └── ai_api/
│       ├── __init__.py
│       ├── main.py         # APIエントリーポイント (Gradio UI)
│       ├── core/
│       │   ├── __init__.py
│       │   └── inference.py  # AI推論ロジック
│       └── config.py       # 設定ファイル
│
└── tests/
    └── core/
        └── test_inference.py
```

### 2.2. 品質管理ツールの設定

- **`pyproject.toml`:** プロジェクトルートに作成し、Ruff (フォーマッター/リンター), Mypy (型チェッカー), Pytest (テストフレームワーク) の設定を記述します。
- **`.pre-commit-config.yaml`:** `pre-commit`を導入し、Gitコミット時に自動でコードチェックが走るように設定します。

### 2.3. 開発環境 (Docker)

- **`requirements.txt`:** 必要なライブラリを記述します。
- **`Dockerfile`:** ローカル開発環境の統一と効率化のために使用します。
    - **注記:** この`Dockerfile`は、あくまでローカル開発用です。デプロイ先のHugging Face Spacesでは、`Dockerfile`は直接使われず、`requirements.txt`に基づいて環境が自動構築されます。

---

## 3. モデル管理

- **モデル選定:** まずは速度とリソースのバランスが良い軽量なモデル（例: `llm-jp/t5-small-japanese-finetuned-sum`）で開発を開始し、必要に応じて、より高品質なモデル（例: `izumi-lab/t5-base-japanese-summary`）への差し替えを検討します。
- **情報記録:** `config.py`には、モデル名だけでなく、モデルサイズ、ライセンス、推論速度の目安などの情報もコメントとして記録し、管理します。

---

## 4. 実装の役割分担

- **`src/ai_api/config.py` (設定担当):** モデルやパラメータの情報を管理します。
- **`src/ai_api/core/inference.py` (AI推論担当):** AIモデルのロードと推論のコアロジックを実装します。
- **`src/ai_api/main.py` (API受付担当):** GradioのUIを定義し、サーバーを起動します。Dockerコンテナ内で外部にAPIを公開するため、`iface.launch(server_name="0.0.0.0")`のように起動オプションを指定します。

---

## 5. テスト戦略

- **ユニットテスト:** `pytest`を使い、`inference.py`内の純粋な関数（AIのコアロジック）をテストします。
- **インテグレーションテスト:** ローカルで起動したGradioサーバーに対し、`requests`ライブラリで実際にAPIリクエストを送り、HTTPステータスコードやレスポンスの形式が期待通りであることを確認するテストも追加します。これにより、APIとしての正常性を保証します。

---

## 6. CI/CDとデプロイ戦略

### 6.1. 推奨ワークフロー
1.  **CI (GitHub Actions):** プルリクエスト作成時やmainブランチへのプッシュ時に、GitHub Actionsを起動し、`pytest`による自動テスト（ユニットテストとインテグレーションテスト）を実行します。
2.  **CD (Hugging Face Hub連携):** テストが成功したコードがmainブランチにマージされたら、Hugging Face Hubの**GitHub連携機能**がそれを検知し、自動的にSpacesにデプロイします。この方法が、Actionsから手動で`git push`するよりシンプルで確実なため推奨されます。

### 6.2. 設定
- **GitHub Actions:** `.github/workflows/ci.yml`にテストのワークフローを定義します。
- **Hugging Face Hub:** Spacesの設定ページで、対象のGitHubリポジトリとブランチを指定して連携を有効化します。

---

## 7. セキュリティと利用制限

- **Abuse対策:** 公開APIが悪用されるのを防ぐため、Gradioの`queue()`メソッドを利用してリクエストを待ち行列に入れ、同時アクセス数を制限することを検討します。
- **免責事項:** `README.md`に、APIの利用は自己責任であること、生成される内容の正確性を保証しないこと、商用利用に関する制限などを明記しておきます。

---

## 8. 今後の開発プロセス

このドキュメントは、今後の調査や開発の進捗に応じて、継続的に更新・メンテナンスされます。
